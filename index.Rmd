---
title: "Portfolio"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    
---

```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(ggplot2)
library(spotifyr)
library(compmus)
library(plotly)
library(cowplot)

corpus_features <- get_playlist_audio_features("", '6iRQ0PFwwV932O3MjgkI5g')
```


### Fifth visualisation
```{r}
lrpd <- get_tidy_audio_analysis("57V4uc2b2diZ4RPHXWecb9")

lrpd |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

***
One of the artists featured most in my corpus is Hozier, and one of his songs that is typical for this corpus is 'Like Real People Do', which is visualised here in the form of a tempogram. It shows that the tempo, which is around 140 BPM, remains fairly constant throughout the song.


### Fourth visualisation
```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

twenty_five <-
  get_tidy_audio_analysis("5ihS6UUlyQAfmp48eSkxuQ") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***
Another song that is typical for my corpus is 'Landslide' by Fleetwood Mac, of which a keygram is visible here, showing in which key every section of the song is. The intro and bridge are clearly visible as two bars at the beginning of the song and just before the 100 seconds mark.

### Third visualisation

```{r}
tnwm <-
  get_tidy_audio_analysis("0QZ5yyl6B6utIWkxeBDxQN") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

tnwmplot1 <- tnwm |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()

tnwmch <-
  get_tidy_audio_analysis("0QZ5yyl6B6utIWkxeBDxQN") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

tnwmchplot2 <- tnwmch |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

plot_grid(tnwmplot1, tnwmchplot2, ncol = 1)
```

***
As stated in the introduction, one of the typical songs of this corpus is 'The night we met' by Lord Huron. The first plot is a cepstrogram depicting that song. The summarisation level of this graph is bars, since that provided the clearest patterns. The second plot is a chromagram of the same song.


### Introduction


*Pasted from assignment 1*
My chosen corpus contains songs from playlists found on Spotify titled or closely resembling ‘autumn’. I chose this corpus because a season is not universally associated with (a particular type of) music, and yet there are many playlists about specific seasons. What makes autumn especially so interesting is that despite it being the most disliked season of the four (at least in my experience), it has so many playlists, many of which do not contain sad music (as one would expect in a playlist about a least favourite season). There are also no direct reasons to create a playlist for this season, like going on a vacation could be a reason to create a summer playlist, or Christmas to create a winter playlist.

The playlists incorporated in the corpus are bound to differ at least a little, since different people from different backgrounds who know and like different songs created them, but significant trends should be discernible. For example, I expect the energy of the songs to be similar, but the duration could differ.

How representative this corpus is, is hard to say. Playlists like these also exist on other music streaming platforms like YouTube, and these can either be incredibly similar or very different, so it is difficult to judge the representativeness.

Some very typical songs for this corpus include The Night We Met by Lord Huron, Cardigan by Taylor Swift, I Wanna Be Yours by the Arctic Monkeys, Yellow by Coldplay and Sweater Weather by The Neighbourhood. These are typical because they appear in several of the playlists I used and looked at for my corpus, indicating that all of the creators of the playlists thought of the same song, so the song must really fit the criteria.

Tabs {data-width=350}
-----------------------------------------------------------------------

### First visualisation

```{r}
valencexdanceability <- ggplot(corpus_features, aes(x=valence, y=danceability)) +
  geom_point(alpha=0.3, color='darkgreen')

valencexdanceability
```

***
This graph depicts the valence on the x-axis and the danceability on the y-axis. It shows that the happier the song, the higher the danceability, which was to be expected. However, is also a big cluster of songs that are classified as having a low valence that still score somewhat high on the danceability scale. Therefore, one of the assumptions that can be made is that some 'autumn' songs can be classified as negative sounding with still some level of danceability.


### Second visualisation

```{r}
## All too well t version
tversion <-
  get_tidy_audio_analysis("5enxwA8aAbwZbf5qCHORXi") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
## All too well sad girl autumn version
sgaversion <-
  get_tidy_audio_analysis("1n3b9Eekoy3S9ZSZ5DmTW1") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

compmus_long_distance(
  tversion |> mutate(pitches = map(pitches, compmus_normalise, "manhattan")),
  sgaversion |> mutate(pitches = map(pitches, compmus_normalise, "manhattan")),
  feature = pitches,
  method = "euclidean"
) |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "All too well Taylor's version", y = "All too well sad girl autumn version") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL)
```

***
This graph depicts two performances of 'All too well' by Taylor Swift.

